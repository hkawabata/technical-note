# 概観

## 状況設定

大前提として、**マルコフモデル** を仮定：将来の**状態（State）** $s_{t+1}$ は現在の状態 $s_t$ だけに依存して決まり、それより過去の状態 $s_{t-1},s_{t-2},\cdots$ には寄らない。

ある時点の状態 $s$ において、次の**行動（Action）** $a$ をあるルールに基づいて決定し、実行する。
これにより状態 $s$ は新たな状態 $s'$ に**遷移（Transition）** する。
状態が $s$ から $s'$ に遷移すると、**即時報酬（Reward）** $R(s,s')$ を受け取ることができる。

**【不確定な要素】**

一般に、状態 $s$ において行動 $a$ を取ったときの遷移先の状態 $s'$ は確定しておらず、確率的に決まる。以後、この遷移確率を条件付き確率 $T(s'\vert s,a)$ で表す。


## 強化学習の目的

$s_t \to s_{t+1} \to s_{t+2} \to \cdots$ と状態の遷移を繰り返す中で、報酬 $R$ の総和 $G$ を最大化すること。

時刻 $t$ 以降、終了時刻 $T$ まで状態遷移を繰り返したときの報酬の総和を $G_t$ とすると、

$$
G_t = R(s_t,s_{t+1}) + R(s_{t+1},s_{t+2}) + R(s_{t+2},s_{t+3}) + \cdots + R(s_{T-1},s_T)
$$

これを直接最大化できれば一番嬉しいが、前述の通り、状態 $s$ で行動 $a$ を取った結果遷移する状態 $s'$ は確率的に決まるため、$s_{t+1},s_{t+2},\cdots,s_{T}$ は不確定。  
→ **未来の報酬は推測して見積もるしかない**

推測の精度は未来になるほど不確かになるので、不確かさを表す **割引率** $\gamma\ (0 \lt \gamma \lt 1)$ を導入して、

$$
G_t = R(s_t,s_{t+1}) + \gamma\ R(s_{t+1},s_{t+2}) + \gamma^2\ R(s_{t+2},s_{t+3}) + \cdots + \gamma^{T-t-1}\ R(s_{T-1},s_T)
$$

の最大化を目指す。この式は時刻 $t+1$ 以降の報酬の総和 $G_{t+1}$ を用いて漸化式形式に変形できる：

$$
\begin{eqnarray}
    G_t &=&
    R(s_t,s_{t+1}) + \gamma\ \left\{R(s_{t+1},s_{t+2}) + \gamma\ R(s_{t+2},s_{t+3}) + \cdots + \gamma^{T-(t+1)-1}\ R(s_{T-1},s_T) \right\}
    \\ &=&
    R(s_t,s_{t+1}) + \gamma\ G_{t+1}
    \tag{1}
\end{eqnarray}
$$

この「ある状態以降に得られる報酬の総和」を見積もった値を、**価値（Value）** と呼ぶ。


## 方策と Bellman 方程式

強化学習が想定する状況では、状態 $s$ に対して、得られる価値が大きくなるよう、あるルールに基づいて次の行動 $a$ を決定する。この行動選択のルールを**方策（Policy）** $\pi$ と呼ぶ。  
一般には、状態 $s$ に対してただ1つの行動を決定するのではなく、確率的に次の行動を選択することが多い。  
すなわち、方策 $\pi$ の実体 = 状態 $s$ のときに行動 $a$ を選択する条件付き確率 $\pi(a \vert s)$

前節で価値 $G_t$ を定義したのと同様に、状態 $s$ から方策 $\pi$ に基づいて行動を決定したときの価値を $V_\pi(s)$ とすると、

$$
V_\pi(s) = E \left[ R(s,s') + \gamma\ V_\pi(s') \right]
\tag{2}
$$

$E[\cdots]$ は期待値を表す。遷移先の状態 $s'$ が確率的に決まる不確定なパラメータであるため、期待値を取ることでその状態の価値としている。  
$(2)$ 式の解釈：**ある状態の価値 = そこからの遷移で得られる即時報酬と、遷移先状態の価値を併せたもの（の期待値）**

期待値を計算するには、元の状態 $s$ から遷移する可能性がある全ての遷移先状態 $s'$ について、その遷移確率 $P(s,s')$ による重み付けをして和を取れば良い：

$$
V_\pi(s) = \sum_{s'} P(s,s')
    \left\{
        R(s,s') + \gamma\ V_\pi(s')
    \right\}
$$

遷移確率 $P(s,s')$ は、状態 $s$ において取りうるあらゆる行動 $a$ について、その行動が取られ、かつその結果遷移先が $s'$ となる確率の和を取ったものだから、

$$
P(s,s') = \sum_a \pi(a \vert s) T(s' \vert s,a)
$$

以上により、

$$
\begin{eqnarray}
    V_\pi(s)
    &=&
    \sum_{s'} \sum_a \pi(a \vert s) T(s' \vert s,a)
    \left\{
        R(s,s') + \gamma\ V_\pi(s')
    \right\}
    \\ &=&
    \sum_a \pi(a \vert s) \sum_{s'} T(s' \vert s,a)
    \left\{
        R(s,s') + \gamma\ V_\pi(s')
    \right\}
    \tag{3}
\end{eqnarray}
$$

この式を **Bellman 方程式** と呼ぶ。

## 状態価値と行動価値

Bellman 方程式 $(3)$ において、遷移先状態 $s'$ についての和の部分を

$$
Q_\pi(s,a) :=
\sum_{s'} T(s' \vert s,a)
\left\{
    R(s,s') + \gamma\ V_\pi(s')
\right\}
\tag{4}
$$

とおくと、$Q_\pi(s,a)$ は状態 $s$ において行動 $a$ を取ったときに得られる報酬の期待値となる。  
これは言い換えれば「状態 $s$ において行動 $a$ が持つ価値」と考えることもできるので、これを **行動価値** と呼ぶ。  
また、行動価値との対比で、状態 $s$ のみで決まる価値 $V_\pi(s)$ を **状態価値** と呼ぶ。

$(4)$ を Bellman 方程式 $(3)$ に代入すれば、

$$
V_\pi(s) = \sum_a \pi(a \vert s) Q_\pi(s,a)
\tag{5}
$$

$(4),(5)$ は状態価値と行動価値を変換する式になっている。

$(5)$ の $s$ を $s'$ に置き換えて $(4)$ 右辺に代入すれば

$$
Q_\pi(s,a) =
\sum_{s'} T(s' \vert s,a)
\left\{
    R(s,s') + \gamma\ \sum_{a'} \pi(a' \vert s') Q_\pi(s',a')
\right\}
\tag{6}
$$

となり、行動価値で見た漸化式が得られる。


## Policy ベースと Value ベース

- **Value ベースの強化学習**：行動選択の方策 $\pi$ として、「常に価値の期待値が最大となる行動を取る」というものを採用し、行動の評価方法のみを学習する
- **Policy ベースの強化学習**：行動の評価方法だけでなく、行動選択の方策 $\pi$ 自体も学習により更新していく手法

Value ベースの手法の場合、常に価値を最大化する方策となるので、$(3)$ 式は以下の **Bellman 最適方程式** に書き換えられる：

$$
\begin{eqnarray}
    V(s)
    &=&
    \max_a \left\{ \sum_{s'} T(s' \vert s,a)
    \left\{
        R(s,s') + \gamma\ V_\pi(s')
    \right\} \right\}
    \\ &=&
    \max_a \left\{ Q_\pi(s,a) \right\}
\end{eqnarray}
\tag{7}
$$

ここで $\displaystyle \max_a \{ f(a) \}$ は、変数 $a$（ここでは行動）に関する $f(a)$ の最大値を意味する。  
すなわち、行動価値 $Q_\pi$ を最大にするような行動 $a$ を代入する。


## モデルベースとモデルフリー

- **モデルベースの強化学習**：状態遷移確率 $T(s'\vert s,a)$ と即時報酬 $R(s,s')$ が分かっている事が前提
- **モデルフリーの強化学習**：状態遷移確率 $T(s'\vert s,a)$ と即時報酬 $R(s,s')$ が未知であることが前提（やってみないと分からない）


## モデルベース、Value ベース：動的計画法による価値評価の学習（価値反復法）

**価値反復法 (Value iteration)**：状態遷移確率 $T(s'\vert s,a)$ と即時報酬 $R(s,s')$ が既知であるときの Value ベースの強化学習の手法。

取りうる全ての状態の集合を $S$ として、動的計画法により状態価値 $V(s)\ (s \in S)$ を計算する。

### 理論（考え方）

まず、すべての状態 $s \in S$ について、状態価値の初期値 $V^0(s)=0$ を設定する。  
強化学習が想定する状況では、状態の遷移を繰り返すことで報酬を得ていくので、$V^0(s)$ は **「各状態 $s$ から0ステップ先までの遷移した（つまり1度も遷移しない）ときの報酬」** と言い換えることができる。

次に、Bellman 最適方程式 $(7)$ を少し書き換えた以下の漸化式を考える：

$$
V^{i+1}(s) =
\max_a \left\{ \sum_{s'} T(s' \vert s,a)
\left\{
    R(s,s') + \gamma\ V^i(s')
\right\}
\right\}
\tag{7'}
$$

これを使って $V^0(s)$ から $V^1(s)$ を計算すると、右辺の $V^i(s')$ 部分はゼロになるので、

$$
V^1(s) =
\max_a \left\{ \sum_{s'} T(s' \vert s,a)
\left\{
    R(s,s')
\right\}
\right\}
$$

式の通り、$V^1(s)$ は **「各状態 $s$ から1ステップ先まで遷移したときに得られる報酬期待値の最大値」** となっている。

次に $V^1(s)$ から $V^2(s)$ を計算すると、

$$
V^2(s) =
\max_a \left\{ \sum_{s'} T(s' \vert s,a)
\left\{
    R(s,s') + \gamma\ V^1(s')
\right\}
\right\}
$$

右辺について、
- $R(s,s')$ は $s\to s'$ の遷移で得られる即時報酬
- $V^1(s')$ は状態 $s'$ から1ステップだけ遷移したときの報酬期待値の最大値

であるから、$V^2(s)$ は **「各状態 $s$ から2ステップ先まで遷移したときに得られる報酬期待値の最大値」** となる。

同様に漸化式 $(7')$ から $V^3(s),V^4(s),V^5(s),\cdots$ を計算していくと、$V^i(s)$ は **「各状態 $s$ から $i$ ステップ先まで遷移したときに得られる報酬期待値の最大値」** となることが分かる。

$i\to \infty$ とすれば状態価値の定義そのものとなるが、現実的には無限回の計算はできないので、値がある程度収束した時点で計算を終了する必要がある。

計算を繰り返すほど $V^i(s)$ の値が正しい状態価値 $V(s)$ に近づいていくことは数学的に証明されている（らしい）ので、各計算ステップで

$$
\Delta (s) := \vert V^i(s) - V^{i-1}(s) \vert
$$

を計算し、全ての $s$ について $\Delta (s) \lt \varepsilon$となったら処理を終了し、最後の $V^i(s)$ を状態価値 $V(s)$ として採用する（$\varepsilon$ は収束条件を設定するための微小なハイパーパラメータ）。


### アルゴリズム

前節の議論から得られた計算手法を整理する。

1. $i \gets 0$
2. すべての状態 $s \in S$ について、初期値 $V^0(s)=0$ を設定
3. $i \gets i+1$
4. すべての $s \in S$ について、漸化式 $(7')$ により $V^{i-1}(s)$ から $V^i(s)$ を計算
5. $\Delta(s) := \vert V^i(s) - V^{i-1}(s) \vert$ の値を求める
6. すべての $s$ について $\Delta(s) \lt \varepsilon$ となったら反復処理を終了して7へ、そうでなければ3へ戻る
7. 最後に計算した $V^i(s)$ の値を各状態の価値とみなす


### 実装

迷路の例。

{% gist 75c0f582b9d50c1944c664916ca6bf00 ~settings.py %}

{% gist 75c0f582b9d50c1944c664916ca6bf00 ~value-iteration-maze.py %}

```python
# 問題のセットアップ
states = [
    State(0, 0), State(0, 1), State(0, 2), State(0, 3, 1.0, is_goal=True),
    State(1, 0), State(1, 2), State(1, 3, -1.0, is_goal=True),
    State(2, 0), State(2, 1), State(2, 2), State(2, 3)
]
actions = [Action(0, 1), Action(0, -1), Action(1, 0), Action(-1, 0)]
environment = Environment(states, actions)
# シミュレーション & 結果の描画
planner = ValueBasePlanner(environment)
planner.plan()
V, A = planner.V, planner.A
draw_result(environment, V, A)
```

報酬最大のゴールでなくても、割引率の影響で近くのゴールが優先される効果が働く：

```python
states = [
    State(0, 0), State(0, 1), State(0, 2), State(0, 3, 1.0, is_goal=True),
    State(1, 0), State(1, 2), State(1, 3, -1.0, is_goal=True),
    State(2, 0), State(2, 1), State(2, 2), State(2, 3),
    State(3, 0, 0.9, is_goal=True), State(3, 1), State(3, 2), State(3, 3),
    State(4, 0), State(4, 1), State(4, 3)
]
actions = [Action(0, 1), Action(0, -1), Action(1, 0), Action(-1, 0)]
environment = Environment(states, actions)
planner = ValueBasePlanner(environment)
planner.plan(gamma=0.9)
V, A = planner.V, planner.A
draw_result(environment, V, A)
```

ゴール以外に報酬ポイントがあると、永遠に往復して何度も報酬を獲得するのが正解になってしまう：

```python
states = [
    State(0, 0), State(0, 1), State(0, 2), State(0, 3, 1.0, is_goal=True),
    State(1, 0), State(1, 2), State(1, 3, -1.0, is_goal=True),
    State(2, 0, reward=0.3), State(2, 1), State(2, 2), State(2, 3)
]
actions = [Action(0, 1), Action(0, -1), Action(1, 0), Action(-1, 0)]
environment = Environment(states, actions)
# シミュレーション & 結果の描画
planner = ValueBasePlanner(environment)
planner.plan()
V, A = planner.V, planner.A
draw_result(environment, V, A)
```



## モデルベース、Policy ベース：動的計画法による戦略の学習（方策反復法）

**方策反復法 (Policy iteration)**：状態遷移確率 $T(s'\vert s,a)$ と即時報酬 $R(s,s')$ が既知であるときの Policy ベースの強化学習の手法。

取りうる全ての状態、行動の集合を $S, A$ として、動的計画法により状態価値 $V(s)\ (s \in S)$ および方策 $\pi(a\vert s)\ (s \in S,\ a \in A)$ を学習する。

方策としては以下のようなものがあり、状況に応じて選択する：

| 方策                                                                            | 説明  |
| :---------------------------------------------------------------------------- | --- |
| greedy 方策<br>ε-greedy 方策<br>ソフトマックス方策<br>Boltzmann 方策<br>エントロピー正則化方策<br>ソフト方策 |     |


greedy な方策改善を行う場合、常に期待値が最大になるよう行動するので、得られる結果は価値反復法の場合と同じになる。



{% gist 75c0f582b9d50c1944c664916ca6bf00 ~policy-iteration-maze.py %}

```python
states = [
    State(0, 0), State(0, 1), State(0, 2), State(0, 3, 1.0, is_goal=True),
    State(1, 0), State(1, 2), State(1, 3, -1.0, is_goal=True),
    State(2, 0), State(2, 1), State(2, 2), State(2, 3)
]
actions = [Action(0, 1), Action(0, -1), Action(1, 0), Action(-1, 0)]
environment = Environment(states, actions)
# シミュレーション & 結果の描画
planner = PolicyBasePlanner(environment)
planner.plan()
V, policy = planner.V, planner.policy
A = {s:max(policy[s], key=policy[s].get) for s in states}
draw_result(environment, V, A)
```


-------


### 探索→活用の二段階に分ける

```python
import numpy as np
from matplotlib import pyplot as plt

class SlotMachine:
    def __init__(self, p):
        """
        p : 当たる確率
        """
        self.p = p
    
    def pull(self):
        if np.random.rand() < self.p:
            return 1
        else:
            return 0


def simulate(p_slot, T, rate_explore):
    """
    p_slot : スロットが当たる確率（複数）
    T : スロットを回す回数
    rate_explore : スロットを回す回数のうち、最適なアルゴリズムを探すために試行する割合
    """
    n_slot = len(p_slot)
    r_total = 0
    slots = []
    r_slots = []
    for p in p_slot:
        slots.append(SlotMachine(p))
        r_slots.append(0)
    T_explore = int(T * rate_explore / n_slot)
    T_exploit = T - T_explore * n_slot
    for i in range(n_slot):
        for t in range(T_explore):
            r_slots[i] += slots[i].pull()
    r_total += np.sum(r_slots)
    best_slot = slots[np.argmax(r_slots)]
    for t in range(T_exploit):
        r_total += best_slot.pull()
    return r_total


def experiment(p_slot, T, rate_explore, N):
    ave = []
    std = []
    for r in rate_explore:
        reward = []
        for i in range(N):
            reward.append(simulate(p_slot, T, r))
        ave.append(np.mean(reward))
        std.append(np.std(reward))
    plt.xlabel('Rate of Exploration')
    plt.ylabel('Reward')
    plt.plot(rate_explore, ave)
    plt.errorbar
    plt.errorbar(rate_explore, ave, yerr=std, capsize=5, fmt='o', markersize=5, ecolor='black', label='$ave. \pm \sigma$')
    plt.legend()
    plt.grid()
    plt.show()

experiment(p_slot=[0.3, 0.5, 0.7], T=100, rate_explore=np.linspace(0.1, 0.9, 9), N=1000)

experiment(p_slot=[0.4, 0.5, 0.6], T=100, rate_explore=np.linspace(0.1, 0.9, 9), N=1000)

experiment(p_slot=[0.3, 0.4, 0.5, 0.6, 0.7], T=100, rate_explore=np.linspace(0.1, 0.9, 9), N=1000)
```

→「最適な方策」を決めるのが早すぎると、最終的な報酬を最大化できない


### 探索結果を反映して重み付けした確率で選ぶ


### Epsilon-Greedy

ハイパーパラメータ $0 \lt \varepsilon \lt 1$ を導入し、
- 確率 $\varepsilon$ でランダムな腕を選択
- 確率 $1-\varepsilon$ でその時点の最適な腕を選択
    - 最適な腕：例えば、それまでの探索・活用で最も平均報酬が高かった腕

{% gist 3294454bfd0c3f0930a253e7a8b63471 ~epsilon-greedy.py %}

gist: https://gist.github.com/hkawabata/3294454bfd0c3f0930a253e7a8b63471

```python
import numpy as np
from matplotlib import pyplot as plt
import random

class EpsilonGreedy:
    def __init__(self):
        pass
    
    def execute(self, ps, T, eps=0.1, report=True):
        """
        ps     : 報酬 (1点) を得られる確率の配列
                 例) ps = [0.2, 0.4, 0.5] の場合、以下の3つの arm を扱う
                 - 確率0.2で報酬1点が得られる arm (index=0)
                 - 確率0.4で報酬1点が得られる arm (index=1)
                 - 確率0.5で報酬1点が得られる arm (index=2)
        T      : 試行回数
        eps    : 全試行中の探索の割合
        report : 結果のサマリを出力するかどうか
        """
        self.eps = eps
        self.ps = ps
        self.T = T
        self.arm_best = None
        self.selected_arms = []
        self.scores = {}
        self.sum_score = {}
        self.ave_score = {}
        self.p_confidence_width = {}  # 信頼区間の幅
        for arm in range(len(ps)):
            self.scores[arm] = []
            self.sum_score[arm] = []
            self.ave_score[arm] = []
            self.p_confidence_width[arm] = []
        self.reward = []
        self.total_reward = [0]
        for t in range(T):
            self.try_once()
        self.total_reward = self.total_reward[1:]
        if report:
            self.report()
    
    def try_once(self):
        arm = self.select_arm()
        r = self.pull_arm(arm)
        n = len(self.scores[arm]) + 1
        r_sum = r if len(self.sum_score[arm]) == 0 else self.sum_score[arm][-1] + r
        r_ave = r_sum / n
        w = 1.96 * np.sqrt(r_ave*(1-r_ave)/n)  # 95%信頼区間を計算
        self.selected_arms.append(arm)
        self.scores[arm].append(r)
        self.sum_score[arm].append(r_sum)
        self.ave_score[arm].append(r_ave)
        self.p_confidence_width[arm].append(w)
        self.reward.append(r)
        self.total_reward.append(self.total_reward[-1] + r)
        self.set_best_arm()
    
    def select_arm(self):
        if self.arm_best is None or np.random.rand() < self.eps:
            # exploration, 探索
            # 現時点の最適 arm 以外をランダムに選ぶ
            while True:
                arm = random.choice(range(len(self.ps)))
                if arm != self.arm_best:
                    break
        else:
            # exploitation, 活用
            # 現時点の最適 arm を選ぶ
            arm = self.arm_best
        return arm
    
    def pull_arm(self, arm):
        if np.random.rand() < self.ps[arm]:
            return 1
        else:
            return 0
    
    def set_best_arm(self):
        tmp = -1
        self.arm_best = None
        for arm in range(len(self.ps)):
            if len(self.ave_score[arm]) > 0 and self.ave_score[arm][-1] > tmp:
                self.arm_best = arm
                tmp = self.ave_score[arm][-1]
    
    def report(self):
        message = """
        ===== Settings =====
        epsilon           : {}
        arm-probabilities : {}
        epochs            : {}
        
        ===== Result =====
        best arm     : {}
        total reward : {}
        """.format(self.eps, self.ps, self.T, self.arm_best, self.total_reward[-1])
        for arm in range(len(self.ps)):
            message += """
            --- arm {}
            probability   : {}
            number of try : {}
            reward sum    : {}
            reward ave    : {} +/- {} (95% CI)
            """.format(arm, self.ps[arm], len(self.scores[arm]), self.sum_score[arm][-1], self.ave_score[arm][-1], self.p_confidence_width[arm][-1])
        print(message)


ps = [0.4, 0.5, 0.6, 0.7]
eg = EpsilonGreedy()
eg.execute(ps, 10000, 0.3)

# total reward
plt.xlabel('Time')
plt.ylabel('Total Reward')
plt.plot(eg.total_reward, label='Simulation')
plt.plot(np.arange(1, len(eg.total_reward)+1) * max(ps), label='Optimal Arm Only')
plt.legend()
plt.grid()
plt.show()

# 平均値の収束具合（信頼区間）を確認
colors = plt.rcParams['axes.prop_cycle'].by_key()['color']
for arm in range(len(ps)):
    y = np.array(eg.ave_score[arm])
    x = range(len(y))
    w = np.array(eg.p_confidence_width[arm])
    plt.plot(x, y, label='arm {}: $p={}$'.format(arm, ps[arm]))
    plt.axhline(y=ps[arm], linewidth=1, ls='dashed', color=colors[arm])
    plt.fill_between(x, y-w, y+w, alpha=0.5)

plt.xlabel('Number of Try', fontsize=14)
plt.ylabel(r'Reward Average (95% CI)', fontsize=14)
plt.legend()
plt.grid()
plt.show()


for eps in [0.1, 0.2, 0.3, 0.4, 0.5]:
    eg = EpsilonGreedy()
    eg.execute(ps, 300, eps)
    plt.plot(eg.total_reward, label='$\\varepsilon = {}$'.format(eps))

plt.xlabel('Time')
plt.ylabel('Total Reward')
plt.legend()
plt.grid()
plt.show()
```

```python
>>> ps = [0.4, 0.5, 0.6, 0.7]
>>> eg = EpsilonGreedy()
>>> eg.execute(ps, 10000, 0.3)

===== Settings =====
epsilon           : 0.3
arm-probabilities : [0.4, 0.5, 0.6, 0.7]
epochs            : 10000

===== Result =====
best arm     : 3
total reward : 6406
    
    --- arm 0
    probability   : 0.4
    number of try : 987
    reward sum    : 420
    reward ave    : 0.425531914893617 +/- 0.030845833995772797 (95% CI)
    
    --- arm 1
    probability   : 0.5
    number of try : 1021
    reward sum    : 505
    reward ave    : 0.49461312438785504 +/- 0.030668179612555763 (95% CI)
    
    --- arm 2
    probability   : 0.6
    number of try : 1045
    reward sum    : 621
    reward ave    : 0.5942583732057416 +/- 0.02977216135608058 (95% CI)
    
    --- arm 3
    probability   : 0.7
    number of try : 6947
    reward sum    : 4860
    reward ave    : 0.6995825536202678 +/- 0.010780514355071543 (95% CI)
```

![eps-greedy-ave](https://gist.github.com/assets/13412823/92660297-f532-48dd-bb3d-312599a9e5b1)

- 割と早めに信頼区間が収束しているので、腕同士を比べた時に95%信頼区間がハッキリ分離していれば性能が低い方の腕を早めに切り捨てる（それ以降探索しない）という手法も有効そう



# 価値ベースとポリシーベース

問題設定：迷路ゲーム

- 0：通路


```python
import numpy as np
from enum import Enum

class Action(Enum):
    UP = 1
    DOWN = 2
    RIGHT = 3
    LEFT = 4


class Environment:
    def __init__(self, maze):
        self.maze_init = np.copy(maze)
        self.reset()
        self.nx, self.ny = np.maze.shape
        self.p_move = 0.8          #
        self.step_penalty = -0.03  # 動くことによるペナルティ
    
    def reset(self):
        self.maze = np.copy(self.maze_init)
    
    def get_transit_prob(self, state, action):
        


class State:
    def __init__(self, x0=0, y0=0):
        self.x0, self.y0 = x0, y0
        self.reset()
    
    def reset(self):
        self.x, self.y = self.x0, self.y0
    
    def update(self, action):
        if action == Action.UP:
            self.y += 1
        elif action == Action.DOWN:
            self.y -= 1
        elif action == Action.RIGHT:
            self.x += 1
        elif action == Action.LEFT:
            self.x -= 1


maze = [
    [0,0,0,2],
    [0,-1,0,1],
    [0,0,0,0]
]

env = Environment(maze)
state = State()


```