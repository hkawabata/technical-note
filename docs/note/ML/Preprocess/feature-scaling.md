---
title: 特徴量のスケーリング
---

# 尺度を揃える必要性

**多くの機械学習アルゴリズムでは、特徴量のスケールが異なる場合に性能が悪化する**。

例えば k 近傍法は未知データと教師データとの距離を利用するため、スケールが大きい特徴量の影響が支配的になる：

![Scaled_vs_NotScaled](https://user-images.githubusercontent.com/13412823/80272524-740fcc00-8705-11ea-8235-2bff16c178c5.png)

また、スケールが揃っていないとトレーニングが中々収束しないということが起こり得る。  
下図は、目的関数の等高線に対して垂直に重みを更新する勾配降下法の例。

![SGD](https://user-images.githubusercontent.com/13412823/80274560-5303a700-8716-11ea-9b1f-d3021caa2138.png)


# スケーリング手法

## 正規化

通常、特徴量を [0, 1] の範囲にスケーリングし直すことを意味する。

$$
x_{norm}^{(i)} = \cfrac{x^{(i)} - x_{min}}{x_{max} - x_{min}}
$$

特徴：
- 有界な区間の値が必要な場合に役立つ
- 問題点として、最大 or 最小に大きく外れた値があった場合に影響を受ける

## 標準化

平均値が0、標準偏差が1となるように変換する。

$$
x_{std}^{(i)} = \cfrac{x^{(i)} - \overline{x}}{\sigma_x}
$$

- $$\overline{x}$$: 特徴量の平均値
- $$\sigma_x$$: 特徴量の標準偏差

特徴：
- 外れ値の悪影響を受けにくくなる
- 変換後も、外れ値に関する情報（どの程度ほかデータからずれているのか）が保持される

| $$x$$ | $$x_{norm}$$ | $$x_{std}$$ |
| :-- | :-- | :-- |
| 1.0  | 0.000000 | -1.566699 |
| 2.0  | 0.111111 | -1.218544 |
| 3.0  | 0.222222 | -0.870388 |
| 4.0  | 0.333333 | -0.522233 |
| 5.0  | 0.444444 | -0.174078 |
| 6.0  | 0.555556 | 0.174078 |
| 7.0  | 0.666667 | 0.522233 |
| 8.0  | 0.777778 | 0.870388 |
| 9.0  | 0.888889 | 1.218544 |
| 10.0 | 1.000000 | 1.566699 |

![Scaling](https://user-images.githubusercontent.com/13412823/80273043-eda9b900-8709-11ea-99fd-abee8cbd9ee9.png)
