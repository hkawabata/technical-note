---
title: RANSAC
---

# RANSAC とは

RANdom SAmple Consensus.  
外れ値を含むデータから、外れ値の影響を除外して数学モデルのパラメータを学習する手法。

## 流れ

1. 全データサンプル $$n$$ 件のうち $$m$$ 件をランダムに抽出（非復元抽出）
2. 抽出したサンプルを使い、仮モデル $$C_1$$ を学習
3. 全データサンプルにおいて、$$C_1$$ による予測値と実際のデータとの差が $$t$$ 以下であるものを全て選び「正常値」とする
    - 正常値が少なすぎる（閾値を決めておく）場合は1に戻る
4. 全ての正常値を用いてモデル $$C_2$$ を学習
5. 全ての正常値に対して、平均二乗誤差などの指標を用いて $$C_2$$ の性能を評価
6. 1〜5を $$k$$ 回繰り返し、最も性能の良いモデルを最終結果とする

## $$m$$, $$k$$ の決め方

### ランダム抽出件数 $$m$$

$$m$$ がなるべく小さい方が、抽出したサンプルに外れ値が紛れ込みにくくなる。  
最低限、モデルの自由度以上の値に設定すれば良い。

例えば二次関数であれば、$$y = a_0 + a_1 x + a_2 x^2$$ の係数 $$a_0, a_1, a_2$$ の3つを決めれば良いので自由度は3。


### 繰り返し回数 $$k$$

全データ件数 $$n$$、1回のランダム抽出件数 $$m$$ に関して $$n \gg m$$ とする。  
（実際、前節の決め方をすれば通常 $$m$$ は $$n$$ に比べて十分小さくなる）

全データサンプル内の外れ値の割合を $$r_{\rm out}$$ とすると、1回のランダムサンプリングにおいて抽出した $$m$$ 件に外れ値が含まれない確率は、

$$
(1 - r_{\rm out})^m
$$

逆に外れ値が含まれる確率は

$$
1 - (1 - r_{\rm out})^m
$$

よって、$$k$$ 回のランダムサンプリングにおいて「少なくとも1回は外れ値が含まれない」確率は、

$$
1 - ( 1 - (1 - r_{\rm out})^m )^k
$$

**この確率をなるべく高くしたい**。$$p_0$$ 以上という条件を課すと、

$$
\begin{eqnarray}
&p_0 \le 1 - ( 1 - (1 - r_{\rm out})^m )^k \\
\Longleftrightarrow\ & ( 1 - (1 - r_{\rm out})^m )^k \le 1 - p_0 \\
\Longleftrightarrow\ & k \log ( 1 - (1 - r_{\rm out})^m ) \le \log (1 - p_0)
\end{eqnarray}
$$

対数の中は1より小さいから、対数は負になる。よって

$$
k \ge \cfrac{\log (1 - p_0)}{\log ( 1 - (1 - r_{\rm out})^m )}
$$

$$p_0$$ が大きい方が外れ値を含まない確率が大きくなるが、大きくしすぎると必要な試行回数 $$k$$ も跳ね上がる。

パラメータ $$$$ を色々変えたときの $$k$$ を以下に示す。

![Optimal k](https://user-images.githubusercontent.com/13412823/81166315-7fe57300-8fce-11ea-9dd3-bfe1b77a2f24.png)


## 実装

最小二乗法による多項式関数への回帰問題を想定。

### コード

最小二乗法：

{% gist fbb64ace37a9d5a68810439062166abf least-square.py %}

RANSAC：

{% gist fbb64ace37a9d5a68810439062166abf ransac.py %}

### 動作確認

シンプルな最小二乗法と RANSAC の結果を比較。

{% gist fbb64ace37a9d5a68810439062166abf ~fit-ransac.py %}

![RANSAC](https://user-images.githubusercontent.com/13412823/81139837-2c5d3000-8fa2-11ea-98f1-c42c38c599b6.png)
